# Задание 1. Исследование моделей и инфраструктуры

Так как подобная задача уже решалась мной в рамках рабочих процессов, в компании запущен проект автоматизации первой линии service desk. В отчёте описываются впечатления от разработки ИИ-агентов в локальной среде и их последующего деплоя на серверах компании. Согласно внутренним политикам безопасности, использование внешних сервисов, в которые могут попасть критичные данные компании, контрагентов и сотрудников, не рекомендуется.

В текущий момент, из-за тестовых нагрузок, принято решение временно отказаться от генеративной LLM в пользу обученного RAG-бота.

Домашнее задание выполнено и описано на базе реального рабочего кейса.



## 1. Сравнение LLM-моделей  
в таблице ниже представлены исследования

___
| Название | Лицензия / Цена | CPU / GPU | Мин. VRAM, GB | Docker | Kubernetes | Поддержка оффлайн инференса | Поддержка локального обучения (LoRA) | Требует обучения для бизнес-качественного ответа | Ограничения по железу | Примечание |
|----------|------------------|-----------|---------------|--------|------------|------------------------------|--------------------------------------|-----------------------------------------------|-----------------------|------------|
| TinyLlama 1.1B | Apache 2.0 / Бесплатно | CPU/GPU | 2 | ✓ | ✓ | Полностью оффлайн | LoRA локально | почти всегда требует доп. обучения | GTX 1050Ti+ (4GB) с 4-bit / CPU ок (медленно) | Для очень слабых машин |
| Phi-3-mini (3.8B) | Apache 2.0 / Бесплатно | CPU/GPU | 4 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для сложных кейсов | GTX 1050Ti+ / GTX 1650, 4–6GB VRAM, CPU ok (с задержками) | Лёгкая модель, минимум ресурсов |
| Mistral 7B Instruct | Apache 2.0 / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для лучшего доменного качества | RTX 2060+ / RTX 3060 12GB (рекоменд.), CPU медленно | Отлично подходит для начала |
| Qwen 1.5 7B Chat | Перmissивная / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для лучшего доменного качества | RTX 2060+ / RTX 3060 12GB+, CPU медленно | Отлично поддерживает русский язык |
| Command R+ | Apache 2.0 / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн | LoRA локально | уже очень хорошо для структурированного вывода | RTX 3060 / 3070 8–12GB+, CPU медленно | Отлично для структурированного вывода |
| Gemma 2B / 7B | Apache 2.0 / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для бизнес-задач | GTX 1660 / RTX 2060+, 8–12GB VRAM, CPU медленно | Чистый open-source от Google |
| Nous Hermes 2 (на Mistral) | Apache 2.0 / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для высокой точности | RTX 3060 12GB+, идеально 3070 / 4060, CPU медленно | Хорошо адаптирован под чат |
| AI Journey 5B | Sber Open License / Бесплатно | CPU/GPU | 12 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для высокой точности в домене | RTX 2060 Super / 3060 (12GB), минимум, CPU долго | Новое, компактное |
| YaLM 8B | Yandex Open License / Бесплатно | CPU/GPU | 16 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для высокой точности в домене | RTX 3060 12GB (очень плотная квантизация), лучше 3070+ / 4060+ | Хорошее понимание русского |
| Mixtral 8x7B | Apache 2.0 / Бесплатно | GPU | 24 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для лучшего доменного качества | RTX 3090 24GB минимум (FP16), 4080+ с квантизацией, A100/A6000 для продакшена | MoE, требует больше VRAM |
| SberGPT 13B | Sber Open License / Бесплатно | CPU/GPU | 24 | ✓ | ✓ | Полностью оффлайн | LoRA локально | желательно для высокой точности в домене | RTX 3090 24GB+ минимум, 3080Ti 12GB (4-bit), CPU тяжело | Отлично для русского языка, гос/корп сценарии |
| Sber ruGPT-3 XL (13B) | Sber Open License / Бесплатно | CPU/GPU | 24 | ✓ | ✓ | Полностью оффлайн | LoRA локально | почти всегда требует обучения | RTX 3090 24GB+, 3080Ti 12GB (4-bit), CPU очень тяжело | Старое, не инструкционное |
| **YandexGPT-5-Lite-8B-instruct (Q4_K_M)** | Yandex Open License / Бесплатно | CPU/GPU | 8 | ✓ | ✓ | Полностью оффлайн (через Ollama) | Ограничено (возможно через адаптацию) | **да, требуется для снижения галлюцинаций** | RTX 2060+ / RTX 3060 12GB рекоменд., CPU возможно с высокой задержкой | Хорошо следует инструкциям, но склонна к галлюцинациям без донастройки |



### 1.1. Рассматриваемые варианты
Рассматривались только варианты, способные работать в оффлайн-режиме на базе Ollama. Кандидатами для отбора стали следующие модели:

* Sber ruGPT-3 XL (13B)
* YandexGPT-5-Lite-8B-instruct-Q4_K_M 

Выбор данных моделей обусловлен тем, что они разработаны российскими компаниями и соответствуют требованиям заказчика по происхождению и контролю технологий.

В результате в качестве основного обрабатывающего модуля, с учётом заявленных заказчиком потребностей, была выбрана модель YandexGPT-5-Lite-8B-instruct-Q4_K_M.

### 1.2. Качество ответов
В ходе тестовой эксплуатации было выявлено, что модель склонна к сильным галлюцинациям и добавляет собственные интерпретации при создании тикетов вместо точной передачи содержимого письма. В настоящее время ведутся работы по доработке промпта, который передаётся модели вместе с телом письма, с целью ограничения произвольных дополнений и повышения точности извлекаемой информации.

### 1.3. Скорость работы
В среднем на локальной машине разработчика с GPU NVIDIA 5060 Ti время ответа модели составляет около 10 секунд.
### 1.4. Стоимость владения и использования

пока что инфраструктура и ее стоимость не расчитывалась тк 

### 1.5. Удобство и простота развёртывания

Для развёртывания потребовалось создать достаточно объёмную инфраструктуру: Ollama была запущена в контейнере, а также реализован shell-скрипт, отвечающий за копирование модели в рабочую директорию и проверку её работоспособности перед запуском инференса.

### 1.6. Вывод по LLM-моделям

Данная модель требует дополнительного дообучения или корректировки весов для снижения уровня галлюцинаций в рамках рассматриваемого кейса. Альтернативный подход — использовать параллельно отдельную модель, обученную на исторических решённых тикетах, а текущую модель исключить из пайплайна и компенсировать её отсутствие более тщательной ручной предобработкой и очисткой входных данных.

---

## 2. Сравнение моделей эмбеддингов  


### 2.1. Рассматриваемые варианты
В рамках текущего кейса классический трансформер для эмбеддингов не использовался. Вместо этого применяется ручная предподготовка текста: очистка тела письма, удаление подписей и цепочек переписки, нормализация и приведение текста к структурированному виду. Данный подход выбран осознанно, так как основной задачей является снижение шума и стабилизация входных данных до этапа дальнейшей обработки, а не семантический поиск по большой базе знаний.

### 2.2. Скорость создания индекса
| Модель | Наблюдения |
|--------|------------|
| Sentence-Transformers | Индексация требует локальных вычислительных ресурсов, время линейно растёт с объёмом данных. При больших объёмах необходим GPU или значительное время на CPU. |
| OpenAI Embeddings | Высокая скорость за счёт облачной инфраструктуры, однако зависит от сетевой задержки и лимитов API. |

### 2.3. Качество поиска
| Модель | Наблюдения |
|--------|------------|
| Sentence-Transformers | Достаточное качество при доменной адаптации, однако без дообучения модель хуже справляется с техническими тикетами и «шумным» текстом. |
| OpenAI Embeddings | Более стабильное качество «из коробки», лучше работает с неструктурированными текстами и сложными формулировками. |
### 2.4. Стоимость владения и использования
| Модель | Наблюдения |
|--------|------------|
| Sentence-Transformers | Низкая стоимость использования, но требуется собственная инфраструктура (CPU/GPU, хранение, обслуживание). |
| OpenAI Embeddings | Отсутствие затрат на инфраструктуру, но постоянные операционные расходы на API и риски роста стоимости при масштабировании. |
### 2.5. Вывод по моделям эмбеддингов
Для текущего этапа проекта использование эмбеддингов признано необходимым на основании изученного материала. Планируется внедрение Sentence-Transformers, так как данный подход позволяет выполнять инференс локально и полностью соответствует требованиям по безопасности и контролю данных.

## 3. Сравнение векторных баз данных  
*(ChromaDB vs FAISS Milvus  )*

### 3.1. Скорость поиска и индексации
| База данных | Наблюдения |
|-------------|------------|
| ChromaDB | |
| FAISS | |

### 3.2. Сложность внедрения и поддержки
| База данных | Наблюдения |
|-------------|------------|
| ChromaDB | |
| FAISS | |

### 3.3. Удобство в работе
| База данных | Наблюдения |
|-------------|------------|
| ChromaDB | |
| FAISS | |

### 3.4. Стоимость владения (учёт инфраструктуры)
| База данных | Наблюдения |
|-------------|------------|
| ChromaDB | |
| FAISS | |

### 3.5. Вывод по векторным БД
_Обоснование выбора._

---

## 4. Рекомендуемая конфигурация сервера для развёртывания RAG-бота

### 4.1. Минимальная конфигурация
- CPU:  
- RAM:  
- GPU:  
- Storage:  
- Обоснование:  

### 4.2. Рекомендуемая конфигурация
- CPU:  
- RAM:  
- GPU:  
- Storage:  
- Обоснование:  

### 4.3. Конфигурация с запасом на рост
- CPU:  
- RAM:  
- GPU:  
- Storage:  
- Обоснование:  

---
